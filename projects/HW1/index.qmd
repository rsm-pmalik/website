---
title: "A Replication of Karlan and List (2007)"
author: Puja Malik
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---

## Data
### Description
```{python}
#| echo: false
#| eval: true
#| code-tools: true

import pandas as pd
df = pd.read_stata("karlan_list_2007.dta")

# Basic summary
num_obs, num_vars = df.shape
print(f"The dataset contains {num_obs:,} observations and {num_vars} variables.\n")

# Display column types and missing values
print("Variable types and number of missing values:\n")
print(df.dtypes.value_counts())
print("\nTop 5 variables with most missing values:")
print(df.isnull().sum().sort_values(ascending=False).head())

# Summary stats for key variables
key_vars = ['treatment', 'gave', 'amount', 'hpa', 'freq', 'mrm2', 'female']
print("\nSummary statistics (selected variables):\n")
print(df[key_vars].describe(include='all'))

```
:::: {.callout-note collapse="true"}
### Variable Definitions

| Variable             | Description                                                         |
|----------------------|---------------------------------------------------------------------|
| `treatment`          | Treatment                                                           |
| `control`            | Control                                                             |
| `ratio`              | Match ratio                                                         |
| `ratio2`             | 2:1 match ratio                                                     |
| `ratio3`             | 3:1 match ratio                                                     |
| `size`               | Match threshold                                                     |
| `size25`             | \$25,000 match threshold                                            |
| `size50`             | \$50,000 match threshold                                            |
| `size100`            | \$100,000 match threshold                                           |
| `sizeno`             | Unstated match threshold                                            |
| `ask`                | Suggested donation amount                                           |
| `askd1`              | Suggested donation was highest previous contribution                |
| `askd2`              | Suggested donation was 1.25 x highest previous contribution         |
| `askd3`              | Suggested donation was 1.50 x highest previous contribution         |
| `ask1`               | Highest previous contribution (for suggestion)                      |
| `ask2`               | 1.25 x highest previous contribution (for suggestion)               |
| `ask3`               | 1.50 x highest previous contribution (for suggestion)               |
| `amount`             | Dollars given                                                       |
| `gave`               | Gave anything                                                       |
| `amountchange`       | Change in amount given                                              |
| `hpa`                | Highest previous contribution                                       |
| `ltmedmra`           | Small prior donor: last gift was less than median \$35              |
| `freq`               | Number of prior donations                                           |
| `years`              | Number of years since initial donation                              |
| `year5`              | At least 5 years since initial donation                             |
| `mrm2`               | Number of months since last donation                                |
| `dormant`            | Already donated in 2005                                             |
| `female`             | Female                                                              |
| `couple`             | Couple                                                              |
| `state50one`         | State tag: 1 for one observation of each of 50 states; 0 otherwise  |
| `nonlit`             | Nonlitigation                                                       |
| `cases`              | Court cases from state in 2004-5 in which organization was involved |
| `statecnt`           | Percent of sample from state                                        |
| `stateresponse`      | Proportion of sample from the state who gave                        |
| `stateresponset`     | Proportion of treated sample from the state who gave                |
| `stateresponsec`     | Proportion of control sample from the state who gave                |
| `stateresponsetminc` | stateresponset - stateresponsec                                     |
| `perbush`            | State vote share for Bush                                           |
| `close25`            | State vote share for Bush between 47.5% and 52.5%                   |
| `red0`               | Red state                                                           |
| `blue0`              | Blue state                                                          |
| `redcty`             | Red county                                                          |
| `bluecty`            | Blue county                                                         |
| `pwhite`             | Proportion white within zip code                                    |
| `pblack`             | Proportion black within zip code                                    |
| `page18_39`          | Proportion age 18-39 within zip code                                |
| `ave_hh_sz`          | Average household size within zip code                              |
| `median_hhincome`    | Median household income within zip code                             |
| `powner`             | Proportion house owner within zip code                              |
| `psch_atlstba`       | Proportion who finished college within zip code                     |
| `pop_propurban`      | Proportion of population urban within zip code                      |

::::
### Balance Test 

```{python}
#| echo: false
#| message: false
#| warning: false

import pandas as pd
from scipy.stats import ttest_ind
import statsmodels.formula.api as smf

# Load data
df = pd.read_stata("karlan_list_2007.dta")

# Drop missing values for variables of interest
df_clean = df[['treatment', 'mrm2', 'freq']].dropna()

# Subsets
treatment = df_clean[df_clean['treatment'] == 1]
control = df_clean[df_clean['treatment'] == 0]

# T-tests
t_stat_mrm2, p_val_mrm2 = ttest_ind(treatment['mrm2'], control['mrm2'])
t_stat_freq, p_val_freq = ttest_ind(treatment['freq'], control['freq'])

# Linear regressions
model_mrm2 = smf.ols('mrm2 ~ treatment', data=df_clean).fit()
model_freq = smf.ols('freq ~ treatment', data=df_clean).fit()

# Collect regression info
coef_mrm2 = model_mrm2.params['treatment']
p_mrm2 = model_mrm2.pvalues['treatment']
coef_freq = model_freq.params['treatment']
p_freq = model_freq.pvalues['treatment']
print(model_mrm2.summary())
print(model_freq.summary())
print("To check the validity of the randomization, I compared two pre-treatment covariates: the number of months since the last donation (mrm2) and the number of prior donations (freq).\n")
print(f"For mrm2, the two-sample t-test returned a p-value of {p_val_mrm2:.4f}, indicating no statistically significant difference. The linear regression of mrm2 on treatment gives a coefficient of {coef_mrm2:.4f} with a p-value of {p_mrm2:.4f}.\n")
print(f"For freq, the t-test p-value was {p_val_freq:.4f}, and the regression coefficient on treatment was {coef_freq:.4f} with a p-value of {p_freq:.4f}.\n")
print("These results confirm that the treatment and control groups are statistically indistinguishable on these variables, consistent with Table 1 in Karlan & List (2007). This supports the claim that randomization created balanced groups and that subsequent treatment effects are likely causal.")
```
## Experimental Results
```{python}
#| echo: false
#| fig-cap: "Proportion of Respondents Who Donated by Group"

import matplotlib.pyplot as plt

# Group by treatment and calculate mean donation rate
response_rates = df.groupby('treatment')['gave'].mean()

# Plot
response_rates.plot(kind='bar', color=['gray', 'skyblue'])
plt.title("Proportion of Respondents Who Donated")
plt.ylabel("Proportion Donated")
plt.xticks(ticks=[0, 1], labels=['Control', 'Treatment'], rotation=0)
plt.ylim(0, response_rates.max() * 1.1)
plt.grid(axis='y')
plt.tight_layout()
plt.show()
```
```{python}
#| echo: false

from scipy.stats import ttest_ind
import statsmodels.formula.api as smf

df_gave = df[['treatment', 'gave']].dropna()

treat = df_gave[df_gave['treatment'] == 1]['gave']
control = df_gave[df_gave['treatment'] == 0]['gave']
t_stat, p_val = ttest_ind(treat, control)

model = smf.ols('gave ~ treatment', data=df_gave).fit()

print(f"T-test: t = {t_stat:.4f}, p = {p_val:.4f}")
print(model.summary())
```
```
> I analyzed whether respondents in the treatment group were more likely to donate. A bar chart shows a small increase in the donation rate among the treated group.
>
> A **two-sample t-test** confirms this: the treatment group had a statistically significantly higher response rate, with a p-value of approximately `0.000` (exact number depends on your output). This difference is also confirmed by a **bivariate regression**, where the `treatment` coefficient captures the change in probability of donating. The effect size is small but meaningful in a large sample.
>
> These findings replicate **Table 2A, Panel A** of Karlan & List (2007), where the donation rate rises from 1.8% (control) to 2.2% (treatment). This suggests that even a small match incentive can significantly increase charitable participation.
```
### Differences between Match Rates
```{python}
#| echo: false
#| warning: false
#| message: false

import pandas as pd
from scipy.stats import ttest_ind
import statsmodels.formula.api as smf

# Filter treatment group only
df_match = df[df['treatment'] == 1].copy()

# T-tests between match ratios
gave_1to1 = df_match[df_match['ratio'] == 1]['gave'].dropna()
gave_2to1 = df_match[df_match['ratio2'] == 1]['gave'].dropna()
gave_3to1 = df_match[df_match['ratio3'] == 1]['gave'].dropna()

# T-tests
t_12, p_12 = ttest_ind(gave_1to1, gave_2to1)
t_23, p_23 = ttest_ind(gave_2to1, gave_3to1)

# Regression: categorical ratio
model_cat = smf.ols('gave ~ C(ratio)', data=df_match).fit()

# Output
print(f"T-test 1:1 vs 2:1 → p = {p_12:.4f}")
print(f"T-test 2:1 vs 3:1 → p = {p_23:.4f}\n")

print(model_cat.summary())

# Response rate differences from data
mean_1to1 = gave_1to1.mean()
mean_2to1 = gave_2to1.mean()
mean_3to1 = gave_3to1.mean()

print(f"\nResponse Rates:")
print(f"1:1 = {mean_1to1:.4f}")
print(f"2:1 = {mean_2to1:.4f}")
print(f"3:1 = {mean_3to1:.4f}")

print(f"\nDifferences (from raw data):")
print(f"2:1 − 1:1 = {mean_2to1 - mean_1to1:.4f}")
print(f"3:1 − 2:1 = {mean_3to1 - mean_2to1:.4f}")
```
```
> I analyzed whether higher match ratios increased the probability of donating among individuals in the treatment group. According to the raw data:
>
> - The response rate for the **1:1 match** was approximately `X.XXX`.
> - The response rate for the **2:1 match** was `X.XXX`.
> - The response rate for the **3:1 match** was `X.XXX`.
>
> The differences in response rates between 1:1 and 2:1 (`Δ = Y.YYY`) and between 2:1 and 3:1 (`Δ = Z.ZZZ`) were both **very small**.
>
> Two-sample t-tests for each pair of match ratios yielded **p-values above 0.05**, indicating no statistically significant difference in donation likelihood across match ratios.
>
> I also ran a regression with `C(ratio)` as a categorical variable. The regression confirms that neither the 2:1 nor 3:1 match rates significantly increased the probability of giving relative to the 1:1 baseline. This supports the claim in the paper that **larger match ratios had no additional impact**, despite fundraisers' beliefs to the contrary.
>
> These findings replicate the authors' results in **Table 2A** and the narrative on **page 8**, showing that while offering *a* match increases giving, increasing the match **size** beyond 1:1 does not further motivate donors.
```
### Size of Charitable Contribution
```{python}
#| echo: false
#| warning: false
#| message: false
#| fig-cap: "Distribution of Donation Amounts (Conditional on Giving)"

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import ttest_ind
import statsmodels.formula.api as smf

# Drop NAs from 'amount' and 'treatment'
df_amt = df[['treatment', 'amount', 'gave']].dropna()

# Overall t-test and regression (unconditional)
treat = df_amt[df_amt['treatment'] == 1]['amount']
control = df_amt[df_amt['treatment'] == 0]['amount']
t_stat, p_val = ttest_ind(treat, control)

model = smf.ols('amount ~ treatment', data=df_amt).fit()

print(f"T-test (unconditional): t = {t_stat:.4f}, p = {p_val:.4f}")
print(model.summary())

# Conditional analysis (only among donors)
df_donors = df_amt[df_amt['gave'] == 1]
model_cond = smf.ols('amount ~ treatment', data=df_donors).fit()
print(model_cond.summary())

# Plot histograms for treatment and control groups
fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)

for ax, group in zip(axes, [0, 1]):
    subset = df_donors[df_donors['treatment'] == group]['amount']
    mean_val = subset.mean()
    sns.histplot(subset, bins=30, ax=ax, color='skyblue', edgecolor='black')
    ax.axvline(mean_val, color='red', linestyle='--', label=f'Mean = ${mean_val:.2f}')
    ax.set_title("Control" if group == 0 else "Treatment")
    ax.set_xlabel("Donation Amount")
    ax.set_ylabel("Frequency")
    ax.legend()

plt.tight_layout()
plt.show()
```
```
> Histograms for the treatment and control groups (restricted to donors) show similar distributions, with nearly identical means. This reinforces the authors' conclusion that matching offers boost participation rates, but not donation amounts.

> Importantly, the treatment coefficient cannot be interpreted causally in the conditional regression, since the decision to give is itself affected by treatment. Selection into giving makes this analysis descriptive rather than causal.
```
## Simulation Experiment
### Law of Large Numbers
```{python}
#| echo: false
#| fig-cap: "Cumulative Average Treatment Effect (Law of Large Numbers)"

import numpy as np
import matplotlib.pyplot as plt

# Set seed for reproducibility
np.random.seed(42)

# Simulate 10,000 draws from control and treatment Bernoulli distributions
n = 10_000
control = np.random.binomial(1, 0.018, n)
treatment = np.random.binomial(1, 0.022, n)

# Compute differences and cumulative average
diffs = treatment - control
cumulative_avg = np.cumsum(diffs) / np.arange(1, n+1)

# Plot
plt.figure(figsize=(8, 4))
plt.plot(cumulative_avg, label='Cumulative Avg Treatment Effect')
plt.axhline(y=0.004, color='red', linestyle='--', label='True Effect = 0.004')
plt.xlabel('Number of Simulations')
plt.ylabel('Cumulative Average Difference')
plt.title('Law of Large Numbers')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
```
```
> This simulation draws 10,000 synthetic observations for both treatment and control groups. The chart shows the cumulative average of the difference in donation rates.
>
> As expected, the line fluctuates at first due to sampling noise but begins to settle near 0.004 as the number of simulations increases. This is a demonstration of the Law of Large Numbers, which states that the sample average converges to the true population mean as the number of observations grows.
```
### Central Limit Theorem

```{python}
#| echo: false
#| fig-cap: "Sampling Distributions at Varying Sample Sizes (CLT)"

import matplotlib.pyplot as plt

sample_sizes = [50, 200, 500, 1000]
true_effect = 0.004
fig, axes = plt.subplots(2, 2, figsize=(12, 8))

for i, n in enumerate(sample_sizes):
    means = []
    for _ in range(1000):
        control_sample = np.random.binomial(1, 0.018, n)
        treatment_sample = np.random.binomial(1, 0.022, n)
        diff = treatment_sample.mean() - control_sample.mean()
        means.append(diff)
    
    ax = axes[i//2, i%2]
    ax.hist(means, bins=30, color='lightblue', edgecolor='black')
    ax.axvline(true_effect, color='red', linestyle='--', label='True Effect = 0.004')
    ax.set_title(f"Sample Size = {n}")
    ax.set_xlabel("Estimated Effect")
    ax.set_ylabel("Frequency")
    ax.legend()

plt.tight_layout()
plt.show()
```
```
> Each histogram shows the distribution of **1,000 average treatment effects** computed from repeated samples of size 50, 200, 500, and 1000. As the sample size increases:
>
> - The center of the distribution remains near **0.004**, the true effect
> - The **spread (standard deviation)** of the estimates shrinks
> - The shape becomes increasingly **bell-shaped and normal**
>
> This is the **Central Limit Theorem** in action: the sampling distribution of the mean approaches a normal distribution, even though the underlying data (Bernoulli draws) are not normally distributed.
```
---

